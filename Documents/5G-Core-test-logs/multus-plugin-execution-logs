
-------------------------------
Multus CNI plugin configuration
-------------------------------
(https://anywhere.eks.amazonaws.com/docs/tasks/cluster/cluster-multus/)




vagrant@vagrant:~/aether-configs/repo$ k get po -A
NAMESPACE             NAME                                       READY   STATUS             RESTARTS      AGE
aether-sdcore-5g      amf-9df4c46d9-ktjt4                        1/1     Running            0             2m24s
aether-sdcore-5g      ausf-65fcf654c4-tspdb                      1/1     Running            0             2m24s
aether-sdcore-5g      mongodb-59f5c996ff-xv2gx                   0/1     Pending            0             2m26s
aether-sdcore-5g      nrf-769bfdf9c7-7wfg5                       1/1     Running            0             2m24s
aether-sdcore-5g      nssf-867cc4df9b-5zgkx                      1/1     Running            0             2m24s
aether-sdcore-5g      pcf-6bdd87558f-jbxmb                       1/1     Running            0             2m24s
aether-sdcore-5g      simapp-75f74d6747-dchr8                    1/1     Running            0             2m24s
aether-sdcore-5g      smf-65dfcc8c97-dz2rk                       1/1     Running            0             2m24s
aether-sdcore-5g      udm-6bff54b7bd-rk9bh                       1/1     Running            0             2m24s
aether-sdcore-5g      udr-5cdbc8df86-2vpm2                       1/1     Running            0             2m24s
aether-sdcore-5g      webui-d6c4f67c5-gr5gg                      0/1     CrashLoopBackOff   2 (18s ago)   2m24s
cattle-fleet-system   fleet-agent-8588968894-q99cz               1/1     Running            0             2m37s
cattle-fleet-system   fleet-controller-57477b4b8b-6fks7          1/1     Running            0             3m6s
cattle-fleet-system   gitjob-957764fd5-vdnbn                     1/1     Running            0             3m6s
fleet-local           sdcore-5g-3dab2-rkdhm                      0/2     Completed          0             2m45s
kube-system           calico-kube-controllers-74677b4c5f-wrmh7   1/1     Running            0             20m
kube-system           calico-node-whb9f                          1/1     Running            0             20m
kube-system           coredns-565d847f94-2j27f                   1/1     Running            0             22m
kube-system           coredns-565d847f94-qk6s4                   1/1     Running            0             22m
kube-system           etcd-vagrant                               1/1     Running            0             22m
kube-system           kube-apiserver-vagrant                     1/1     Running            0             22m
kube-system           kube-controller-manager-vagrant            1/1     Running            0             22m
kube-system           kube-proxy-gk545                           1/1     Running            0             22m
kube-system           kube-scheduler-vagrant                     1/1     Running            0             22m



1. Install and configure Multus
--------------------------------

vagrant@vagrant:~$ git clone https://github.com/k8snetworkplumbingwg/multus-cni.git && cd multus-cni
Cloning into 'multus-cni'...
remote: Enumerating objects: 39541, done.
remote: Total 39541 (delta 0), reused 0 (delta 0), pack-reused 39541
Receiving objects: 100% (39541/39541), 49.15 MiB | 25.58 MiB/s, done.
Resolving deltas: 100% (18227/18227), done.

vagrant@vagrant:~$ cd multus-cni/
vagrant@vagrant:~/multus-cni$ ll
total 192
drwxrwxr-x 13 vagrant vagrant  4096 Jan 21 01:23 ./
drwxr-xr-x 11 vagrant vagrant  4096 Jan 21 01:24 ../
drwxrwxr-x  5 vagrant vagrant  4096 Jan 21 01:23 cmd/
-rw-rw-r--  1 vagrant vagrant  5347 Jan 21 01:23 CODE_OF_CONDUCT.md
-rw-rw-r--  1 vagrant vagrant  1582 Jan 21 01:23 CONTRIBUTING.md
drwxrwxr-x  3 vagrant vagrant  4096 Jan 21 01:23 deployments/
drwxrwxr-x  3 vagrant vagrant  4096 Jan 21 01:23 docs/
drwxrwxr-x  3 vagrant vagrant  4096 Jan 21 01:23 e2e/
drwxrwxr-x  2 vagrant vagrant  4096 Jan 21 01:23 examples/
drwxrwxr-x  8 vagrant vagrant  4096 Jan 21 01:23 .git/
drwxrwxr-x  4 vagrant vagrant  4096 Jan 21 01:23 .github/
-rw-rw-r--  1 vagrant vagrant   155 Jan 21 01:23 .gitignore
-rw-rw-r--  1 vagrant vagrant  4283 Jan 21 01:23 go.mod
-rw-rw-r--  1 vagrant vagrant   745 Jan 21 01:23 .goreleaser.yml
-rw-rw-r--  1 vagrant vagrant 80072 Jan 21 01:23 go.sum
drwxrwxr-x  2 vagrant vagrant  4096 Jan 21 01:23 hack/
drwxrwxr-x  2 vagrant vagrant  4096 Jan 21 01:23 images/
-rw-rw-r--  1 vagrant vagrant 11358 Jan 21 01:23 LICENSE
-rw-rw-r--  1 vagrant vagrant    63 Jan 21 01:23 NOTICE
drwxrwxr-x 11 vagrant vagrant  4096 Jan 21 01:23 pkg/
-rw-rw-r--  1 vagrant vagrant  5227 Jan 21 01:23 README.md
-rw-rw-r--  1 vagrant vagrant  6224 Jan 21 01:23 .travis.yml
drwxrwxr-x  8 vagrant vagrant  4096 Jan 21 01:23 vendor/

vagrant@vagrant:~/multus-cni$ kubectl apply -f ./deployments/multus-daemonset-thick-plugin.yml
error: the path "./deployments/multus-daemonset-thick-plugin.yml" does not exist

vagrant@vagrant:~/multus-cni$ kubectl apply -f ./deployments/multus-daemonset-thick.yml
customresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io created
clusterrole.rbac.authorization.k8s.io/multus created
clusterrolebinding.rbac.authorization.k8s.io/multus created
serviceaccount/multus created
configmap/multus-daemon-config created
daemonset.apps/kube-multus-ds created


vagrant@vagrant:~/multus-cni$ k get po -A
NAMESPACE             NAME                                       READY   STATUS      RESTARTS      AGE
aether-sdcore-5g      amf-9df4c46d9-ktjt4                        1/1     Running     0             4m10s
aether-sdcore-5g      ausf-65fcf654c4-tspdb                      1/1     Running     0             4m10s
aether-sdcore-5g      mongodb-59f5c996ff-xv2gx                   0/1     Pending     0             4m12s
aether-sdcore-5g      nrf-769bfdf9c7-7wfg5                       1/1     Running     0             4m10s
aether-sdcore-5g      nssf-867cc4df9b-5zgkx                      1/1     Running     0             4m10s
aether-sdcore-5g      pcf-6bdd87558f-jbxmb                       1/1     Running     0             4m10s
aether-sdcore-5g      simapp-75f74d6747-dchr8                    1/1     Running     0             4m10s
aether-sdcore-5g      smf-65dfcc8c97-dz2rk                       1/1     Running     0             4m10s
aether-sdcore-5g      udm-6bff54b7bd-rk9bh                       1/1     Running     0             4m10s
aether-sdcore-5g      udr-5cdbc8df86-2vpm2                       1/1     Running     0             4m10s
aether-sdcore-5g      webui-d6c4f67c5-gr5gg                      1/1     Running     4 (68s ago)   4m10s
cattle-fleet-system   fleet-agent-8588968894-q99cz               1/1     Running     0             4m23s
cattle-fleet-system   fleet-controller-57477b4b8b-6fks7          1/1     Running     0             4m52s
cattle-fleet-system   gitjob-957764fd5-vdnbn                     1/1     Running     0             4m52s
fleet-local           sdcore-5g-3dab2-rkdhm                      0/2     Completed   0             4m31s
kube-system           calico-kube-controllers-74677b4c5f-wrmh7   1/1     Running     0             21m
kube-system           calico-node-whb9f                          1/1     Running     0             21m
kube-system           coredns-565d847f94-2j27f                   1/1     Running     0             24m
kube-system           coredns-565d847f94-qk6s4                   1/1     Running     0             24m
kube-system           etcd-vagrant                               1/1     Running     0             24m
kube-system           kube-apiserver-vagrant                     1/1     Running     0             24m
kube-system           kube-controller-manager-vagrant            1/1     Running     0             24m
kube-system           kube-multus-ds-7ndrz                       1/1     Running     0             11s
kube-system           kube-proxy-gk545                           1/1     Running     0             24m
kube-system           kube-scheduler-vagrant                     1/1     Running     0             24m

vagrant@vagrant:~/multus-cni$ k get po -A | grep multus
kube-system           kube-multus-ds-7ndrz                       1/1     Running     0             25s




2. Create Network Attachment Definition
----------------------------------------


vagrant@vagrant:~/multus-cni$ cat <<EOF | kubectl create -f -
> apiVersion: "k8s.cni.cncf.io/v1"
> kind: NetworkAttachmentDefinition
> metadata:
>    name: ipvlan-conf
> spec:
>    config: '{
>       "cniVersion": "0.3.0",
>       "type": "ipvlan",
>       "master": "eth1",
>       "mode": "l3",
>       "ipam": {
>          "type": "host-local",
>          "subnet": "198.17.0.0/24",
>          "rangeStart": "198.17.0.200",
>          "rangeEnd": "198.17.0.216",
>          "routes": [
>              { "dst": "0.0.0.0/0" }
>          ],
>          "gateway": "198.17.0.1"
>       }
>  }'
> EOF
networkattachmentdefinition.k8s.cni.cncf.io/ipvlan-conf created

vagrant@vagrant:~/multus-cni$ ifconfig
cali30c617e993a: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1480
        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20<link>
        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)
        RX packets 932  bytes 82380 (82.3 KB)
        RX errors 0  dropped 2  overruns 0  frame 0
        TX packets 925  bytes 125451 (125.4 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

...

docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
        ether 02:42:0f:6a:29:77  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 10.0.2.15  netmask 255.255.255.0  broadcast 10.0.2.255
        inet6 fe80::a00:27ff:fe0a:e282  prefixlen 64  scopeid 0x20<link>
        ether 08:00:27:0a:e2:82  txqueuelen 1000  (Ethernet)
        RX packets 1345380  bytes 1955142156 (1.9 GB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 131814  bytes 8987614 (8.9 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.10.5  netmask 255.255.255.0  broadcast 192.168.10.255
        inet6 fe80::a00:27ff:fef4:e831  prefixlen 64  scopeid 0x20<link>
        ether 08:00:27:f4:e8:31  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 14  bytes 1076 (1.0 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.20.5  netmask 255.255.255.0  broadcast 192.168.20.255
        inet6 fe80::a00:27ff:febc:f95d  prefixlen 64  scopeid 0x20<link>
        ether 08:00:27:bc:f9:5d  txqueuelen 1000  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 14  bytes 1076 (1.0 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 439342  bytes 283377639 (283.3 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 439342  bytes 283377639 (283.3 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

tunl0: flags=193<UP,RUNNING,NOARP>  mtu 1480
        inet 192.168.52.128  netmask 255.255.255.255
        tunnel   txqueuelen 1000  (IPIP Tunnel)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

vagrant@vagrant:~/multus-cni$ k get po -A
NAMESPACE             NAME                                       READY   STATUS             RESTARTS      AGE
aether-sdcore-5g      amf-9df4c46d9-ktjt4                        1/1     Running            0             7m30s
aether-sdcore-5g      ausf-65fcf654c4-tspdb                      1/1     Running            0             7m30s
aether-sdcore-5g      mongodb-59f5c996ff-xv2gx                   0/1     Pending            0             7m32s
aether-sdcore-5g      nrf-769bfdf9c7-7wfg5                       1/1     Running            0             7m30s
aether-sdcore-5g      nssf-867cc4df9b-5zgkx                      1/1     Running            0             7m30s
aether-sdcore-5g      pcf-6bdd87558f-jbxmb                       1/1     Running            0             7m30s
aether-sdcore-5g      simapp-75f74d6747-dchr8                    1/1     Running            0             7m30s
aether-sdcore-5g      smf-65dfcc8c97-dz2rk                       1/1     Running            0             7m30s
aether-sdcore-5g      udm-6bff54b7bd-rk9bh                       1/1     Running            0             7m30s
aether-sdcore-5g      udr-5cdbc8df86-2vpm2                       1/1     Running            0             7m30s
aether-sdcore-5g      webui-d6c4f67c5-gr5gg                      0/1     CrashLoopBackOff   5 (80s ago)   7m30s
cattle-fleet-system   fleet-agent-8588968894-q99cz               1/1     Running            0             7m43s
cattle-fleet-system   fleet-controller-57477b4b8b-6fks7          1/1     Running            0             8m12s
cattle-fleet-system   gitjob-957764fd5-vdnbn                     1/1     Running            0             8m12s
fleet-local           sdcore-5g-3dab2-rkdhm                      0/2     Completed          0             7m51s
kube-system           calico-kube-controllers-74677b4c5f-wrmh7   1/1     Running            0             25m
kube-system           calico-node-whb9f                          1/1     Running            0             25m
kube-system           coredns-565d847f94-2j27f                   1/1     Running            0             27m
kube-system           coredns-565d847f94-qk6s4                   1/1     Running            0             27m
kube-system           etcd-vagrant                               1/1     Running            0             27m
kube-system           kube-apiserver-vagrant                     1/1     Running            0             27m
kube-system           kube-controller-manager-vagrant            1/1     Running            0             27m
kube-system           kube-multus-ds-h9zfv                       1/1     Running            0             2m10s
kube-system           kube-proxy-gk545                           1/1     Running            0             27m
kube-system           kube-scheduler-vagrant                     1/1     Running            0             27m


2-1. Verify the configuration
------------------------------

vagrant@vagrant:~/multus-cni$ kubectl get network-attachment-definitions
NAME          AGE
ipvlan-conf   60s

vagrant@vagrant:~/multus-cni$ k describe network-attachment-definitions.k8s.cni.cncf.io ipvlan-conf
Name:         ipvlan-conf
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2023-01-21T01:27:23Z
  Generation:          1
  Managed Fields:
    API Version:  k8s.cni.cncf.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        .:
        f:config:
    Manager:         kubectl-create
    Operation:       Update
    Time:            2023-01-21T01:27:23Z
  Resource Version:  4987
  UID:               e563cfd2-3c53-40df-a031-ec0bff15f68a
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "ipvlan", "master": "eth1", "mode": "l3", "ipam": { "type": "host-local", "subnet": "198.17.0.0/24", "rangeStart": "198.17.0.200", "rangeEnd": "198.17.0.216", "routes": [ { "dst": "0.0.0.0/0" } ], "gateway": "198.17.0.1" } }
Events:    <none>



3. Deploy sample applications with network attachment
------------------------------------------------------

3.1 Create a sample application 1 (app1) with network annotation created in the previous steps:

	vagrant@vagrant:~/multus-cni$ cat <<EOF | kubectl apply -f -
	> apiVersion: v1
	> kind: Pod
	> metadata:
	>   name: app1
	>   annotations:
	>     k8s.v1.cni.cncf.io/networks: ipvlan-conf
	> spec:
	>   containers:
	>   - name: app1
	>     command: ["/bin/sh", "-c", "trap : TERM INT; sleep infinity & wait"]
	>     image: alpine
	> EOF
	pod/app1 created


3.2 Create a sample application 2 (app2) with the network annotation created in the previous step:

	vagrant@vagrant:~/multus-cni$ cat <<EOF | kubectl apply -f -
	apiVersion: v1
	kind: Pod
	metadata:
	  name: app2
	  annotations:
		k8s.v1.cni.cncf.io/networks: ipvlan-conf
	spec:
	  containers:
	  - name: app2
		command: ["/bin/sh", "-c", "trap : TERM INT; sleep infinity & wait"]
		image: alpine
	EOF
	pod/app2 created


3.3 Verify that the additional interfaces were created on these application pods using the defined network attachment:

	vagrant@vagrant:~/multus-cni$ kubectl exec -it app1 -- ip a
	1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
		link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
		inet 127.0.0.1/8 scope host lo
		   valid_lft forever preferred_lft forever
		inet6 ::1/128 scope host
		   valid_lft forever preferred_lft forever
	2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN qlen 1000
		link/ipip 0.0.0.0 brd 0.0.0.0
	4: eth0@if27: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP
		link/ether aa:95:5f:ca:4a:89 brd ff:ff:ff:ff:ff:ff
		inet 192.168.52.147/32 scope global eth0
		   valid_lft forever preferred_lft forever
		inet6 fe80::a895:5fff:feca:4a89/64 scope link
		   valid_lft forever preferred_lft forever
	5: net1@if3: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UNKNOWN
		link/ether 08:00:27:f4:e8:31 brd ff:ff:ff:ff:ff:ff
		inet 198.17.0.200/24 brd 198.17.0.255 scope global net1
		   valid_lft forever preferred_lft forever
		inet6 fe80::800:2700:1f4:e831/64 scope link
		   valid_lft forever preferred_lft forever


	vagrant@vagrant:~/multus-cni$ kubectl exec -it app2 -- ip a
	1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
		link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
		inet 127.0.0.1/8 scope host lo
		   valid_lft forever preferred_lft forever
		inet6 ::1/128 scope host
		   valid_lft forever preferred_lft forever
	2: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN qlen 1000
		link/ipip 0.0.0.0 brd 0.0.0.0
	4: eth0@if28: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1480 qdisc noqueue state UP
		link/ether 0a:74:7f:c9:d9:53 brd ff:ff:ff:ff:ff:ff
		inet 192.168.52.148/32 scope global eth0
		   valid_lft forever preferred_lft forever
		inet6 fe80::874:7fff:fec9:d953/64 scope link
		   valid_lft forever preferred_lft forever
	5: net1@if3: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UNKNOWN
		link/ether 08:00:27:f4:e8:31 brd ff:ff:ff:ff:ff:ff
		inet 198.17.0.201/24 brd 198.17.0.255 scope global net1
		   valid_lft forever preferred_lft forever
		inet6 fe80::800:2700:2f4:e831/64 scope link
		   valid_lft forever preferred_lft forever


3.4 Test the network connectivity across these pods for Multus interfaces:

	vagrant@vagrant:~/multus-cni$ kubectl exec -it app1 -- ping -I net1 198.17.0.201
	PING 198.17.0.201 (198.17.0.201): 56 data bytes
	64 bytes from 198.17.0.201: seq=0 ttl=64 time=0.048 ms
	64 bytes from 198.17.0.201: seq=1 ttl=64 time=0.529 ms
	64 bytes from 198.17.0.201: seq=2 ttl=64 time=0.069 ms
	64 bytes from 198.17.0.201: seq=3 ttl=64 time=0.058 ms
	^C
	--- 198.17.0.201 ping statistics ---
	4 packets transmitted, 4 packets received, 0% packet loss
	round-trip min/avg/max = 0.048/0.176/0.529 ms





