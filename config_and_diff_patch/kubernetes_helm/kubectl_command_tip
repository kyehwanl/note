
=============
Kubectl Tips
=============

 
0. Install kubectl 
	curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
	chmod +x kubectl
	sudo mv kubectl /usr/local/bin/


1. delete all components
    kubectl delete all [--all] [--all-namespaces|-A]
        --all refers to all resources, including uninitialized ones
        --all-namespaces in all all namespaces


2. assign external ip address to service
    kubectl patch svc <service-name> -p '{"spec":{"externalIPs":["192.168.0.193"]}}'


3. ConfigMap - get, edit
    kubectl get configmap upf -n omec -o yaml 
    kubectl edit configmap -n omec upf


4. Helm (manual install)
    helm install -n omec bess-upf ./sdfabric-helm-charts/bess-upf -f ./upf-overrides.yaml

    - ./sdfabric-helm-charts/bess-upf: template directory location
    - upf-overrides.yaml : local override file


5. Statefulset
    kubectl get statefulset -n omec upf -o yaml


6. pod
    kubectl get po -n omec upf-0 -o yaml



--------------------------------
7. kubectl with bash completion
--------------------------------

#!/bin/bash
export KUBECONFIG=$HOME/.kube/config
if [ -f /etc/profile.d/bash_completion.sh ]; then # in case not exist, apt or yum install bash-completion
  source /etc/profile.d/bash_completion.sh # this line is for the case of errors, "bash: _get_comp_words_by_ref: command not found"
  source <(kubectl completion bash)
  alias k=kubectl
  source <(kubectl completion bash | sed s/kubectl/k/g)
fi 



--------------
8. ETCD access
--------------
https://sysdig.com/blog/monitor-etcd/
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
https://blog.palark.com/how-to-modify-etcd-data-of-your-kubernetes-directly-without-k8s-api/

(1) install
sudo apt install etcd-client


(2) commands
  node0 [904]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt version
  etcdctl version: 3.2.17
  API version: 3.2

  node0 [905]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt member list
  a699090a75570c08, started, node0.kyehwanl-144261.bgpsec.emulab.net, https://155.98.36.92:2380, https://155.98.36.92:2379

	where, endpoints ip address is derived from 'kubeadmin join'
	  kubeadm join 155.98.36.92:6443 --token s0w9b6.090zxeo626zdzixb \
			  --discovery-token-ca-cert-hash sha256:bf4879c843bd6d4485f6e723b43aca109b6cf601237753ef582a42e32343d729



-- in version 2, --> BUT not working
  node0 [910]{~/kubernetes}$ sudo etcdctl --endpoints https://155.98.36.92:2379  --cert-file=/etc/kubernetes/pki/etcd/server.crt  --key-file=/etc/kubernetes/pki/etcd/server.key  --ca-file=/etc/kubernetes/pki/etcd/ca.crt ls
  Error:  client: response is invalid json. The endpoint is probably not valid etcd cluster endpoint.



(3) useful commands

node0 [924]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt endpoint status --write-out=table
+---------------------------+------------------+---------+---------+-----------+-----------+------------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+---------------------------+------------------+---------+---------+-----------+-----------+------------+
| https://155.98.36.92:2379 | a699090a75570c08 |   3.5.6 |   28 MB |      true |         2 |      43117 |
+---------------------------+------------------+---------+---------+-----------+-----------+------------+


node0 [925]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only
/registry/apiextensions.k8s.io/customresourcedefinitions/alertmanagers.monitoring.coreos.com
...
/registry/services/endpoints/kube-system/kube-dns
/registry/services/specs/cattle-fleet-system/gitjob
/registry/services/specs/cattle-system/rancher
/registry/services/specs/cattle-system/rancher-webhook
/registry/services/specs/cattle-system/webhook-service
/registry/services/specs/cert-manager/cert-manager
/registry/services/specs/cert-manager/cert-manager-webhook
/registry/services/specs/default/kubernetes
/registry/services/specs/kube-system/kube-dns
...(truncated)..


(4) if '--keys-only' is not specified, all the data can be shown in forms of JSON
  node0 [932]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix  > etcd_data

...
/registry/management.cattle.io/nodes/local/machine-zqzkq                                                                                                                                                                                                                    {"apiVersion":"management.cattle.io/v3","kind":"Node","metadata":{"annotations":{"cleanup.cattle.io/user-node-remove":"true","lifecycle.cattle.io/create.node-controller":"true","management.cattle.io/nodesyncer":"true"},"creationTimestamp":"2023-01-10T16:00:32Z","finalizers":["controller.cattle.io/node-controller"],"generateName":"machine-","generation":35,"labels":{"cattle.io/creator":"norman","management.cattle.io/nodename":"node0.kyehwanl-144261.bgpsec.emulab.net"},"managedFields":[{"apiVersion":"management.cattle.io/v3","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cleanup.cattle.io/user-node-remove":{},"f:lifecycle.cattle.io/create.node-controller":{},"f:management.cattle.io/nodesyncer":{}},"f:finalizers":{".":{},"v:\"controller.cattle.io/node-controller\"":{}},"f:generateName":{},"f:labels":{".":{},"f:cattle.io/creator":{},"f:management.cattle.io/nodename":{}}},"f:spec":{".":{},"f:controlPlane":{},"f:customConfig":{},"f:desiredNodeTaints":{},"f:displayName":{},"f:etcd":{},"f:imported":{},"f:internalNodeSpec":{".":{},"f:podCIDR":{},"f:podCIDRs":{}},"f:metadataUpdate":{".":{},"f:annotations":{},"f:labels":{}},"f:nodePoolName":{},"f:requestedHostname":{},"f:worker":{}},"f:status":{".":{},"f:conditions":{},"f:internalNodeStatus":{".":{},"f:addresses":{},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{},"f:daemonEndpoints":{".":{},"f:kubeletEndpoint":{".":{},"f:Port":{}}},"f:nodeInfo":{".":{},"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}},"f:limits":{".":{},"f:memory":{}},"f:nodeAnnotations":{".":{},"f:kubeadm.alpha.kubernetes.io/cri-socket":{},"f:node.alpha.kubernetes.io/ttl":{},"f:projectcalico.org/IPv4Address":{},"f:projectcalico.org/IPv4IPIPTunnelAddr":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:nodeLabels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}},"f:nodeName":{},"f:requested":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}}}},"manager":"rancher","operation":"Update","time":"2023-01-10T16:00:32Z"}],"name":"machine-zqzkq","namespace":"local","uid":"9c360f3d-1551-457a-a952-58641b65f904"},"spec":{"controlPlane":false,"customConfig":null,"desiredNodeTaints":null,"displayName":"","etcd":false,"imported":false,"internalNodeSpec":{"podCIDR":"192.168.0.0/24","podCIDRs":["192.168.0.0/24"]},"metadataUpdate":{"annotations":{},"labels":{}},"nodePoolName":"","requestedHostname":"node0.kyehwanl-144261.bgpsec.emulab.net","worker":true},"status":{"conditions":[{"lastTransitionTime":"2023-01-10 15:55:19 +0000 UTC","lastUpdateTime":"2023-01-10 18:01:18 +0000 UTC","message":"kubelet is posting ready status. AppArmor enabled","reason":"KubeletReady","status":"True","type":"Ready"},{"lastUpdateTime":"2023-01-10T16:00:49Z","message":"registered with kubernetes","status":"True","type":"Registered"},{"lastUpdateTime":"2023-01-10T16:00:32Z","status":"True","type":"Provisioned"}],"internalNodeStatus":{"addresses":[{"address":"155.98.36.92","type":"InternalIP"},{"address":"node0.kyehwanl-144261.bgpsec.emulab.net","type":"Hostname"}],"allocatable":{"cpu":"32","ephemeral-storage":"15098469556","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"65767036Ki","pods":"110"},"capacity":{"cpu":"32","ephemeral-storage":"16382888Ki","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"65869436Ki","pods":"110"},"conditions":[{"lastHeartbeatTime":"2023-01-10T18:01:18Z","lastTransitionTime":"2023-01-10T15:55:19Z","message":"kubelet is posting ready status. AppArmor enabled","reason":"KubeletReady","status":"True","type":"Ready"}],"daemonEndpoints":{"kubeletEndpoint":{"Port":10250}},"nodeInfo":{"architecture":"amd64","bootID":"886647ff-bacb-49e9-b60c-d7a8f40074cb","containerRuntimeVersion":"containerd://1.5.9","kernelVersion":"4.15.0-169-generic","kubeProxyVersion":"v1.24.9","kubeletVersion":"v1.24.9","machineID":"01a16d77f3024ba2a01a2b0e2ffd560d","operatingSystem":"linux","osImage":"Ubuntu 18.04.1 LTS","systemUUID":"4C4C4544-0056-4710-8052-B6C04F313832"}},"limits":{"memory":"340Mi"},"nodeAnnotations":{"kubeadm.alpha.kubernetes.io/cri-socket":"unix:///var/run/containerd/containerd.sock","node.alpha.kubernetes.io/ttl":"0","projectcalico.org/IPv4Address":"10.10.1.1/24","projectcalico.org/IPv4IPIPTunnelAddr":"192.168.243.64","volumes.kubernetes.io/controller-managed-attach-detach":"true"},"nodeLabels":{"beta.kubernetes.io/arch":"amd64","beta.kubernetes.io/os":"linux","kubernetes.io/arch":"amd64","kubernetes.io/hostname":"node0.kyehwanl-144261.bgpsec.emulab.net","kubernetes.io/os":"linux","node-role.kubernetes.io/control-plane":"","node.kubernetes.io/exclude-from-external-load-balancers":""},"nodeName":"node0.kyehwanl-144261.bgpsec.emulab.net","requested":{"cpu":"1100m","memory":"240Mi","pods":"19"}}}                                                                                                                                                     


 Then, you can try to find /registry/configmaps, deployments, events, pods ...

  node0 [944]{~/kubernetes}$ sudo ETCDCTL_API=3 etcdctl --endpoints https://155.98.36.92:2379  --cert=/etc/kubernetes/pki/etcd/server.crt  --key=/etc/kubernetes/pki/etcd/server.key  --cacert=/etc/kubernetes/pki/etcd/ca.crt get /registry/pods/kube-system/ --prefix 






------------------------------------
9. Kubernetes pod stuck terminating
------------------------------------
following command to delete the POD forcefully:
  kubectl delete pods <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>
  
  kubectl delete --force pods -n omec --all







------------------------------------
10. Kubernetes checking Event
------------------------------------

- getting events with Watch (-w) 
 kubectl get events -w -n omec | grep upf






-----------------------------------
11. Where the pods logs are stored 
-----------------------------------
/var/log/pods/

  onfadmin@5g1-comp2:~$ ll /var/log/pods/
  calico-system_calico-kube-controllers-7f7959b5db-99hdt_544ff2f5-1581-4101-a749-474b7125092b/
  calico-system_calico-node-pkscj_09625468-9dd8-4224-9833-342040a8517d/
  calico-system_calico-typha-64b7764f4-2gbzs_b0abc9a0-8624-4266-ad18-7cb868c54e5b/
  kube-system_cloud-controller-manager-5g1-comp2_1a84611ed06607ed8a51e65d936a6ff0/
  kube-system_etcd-5g1-comp2_e18aa5e5b83a5a3c56d78e4054612394/
  kube-system_helm-install-rke2-calico-5fkkw_7cf64442-6896-4274-82da-9913a535bb3c/
  kube-system_helm-install-rke2-calico-crd-mvhxh_512e4056-da3a-4911-aebd-5aaa3e7123e2/
  kube-system_helm-install-rke2-coredns-rgf9h_4a19b85a-fbcf-4afa-8f2e-10e7c5dd57c5/
  kube-system_helm-install-rke2-ingress-nginx-p9bqd_019dce1b-d533-4a6c-8623-3898d133435a/
  kube-system_helm-install-rke2-metrics-server-ljxhx_2600b2c2-ab28-47a7-9fb6-375ad9cf845d/
  kube-system_helm-install-rke2-multus-5xjgk_2f43f8fe-14b0-4fe1-a835-0c8f879a2ed9/
  kube-system_kube-apiserver-5g1-comp2_4874a08227e8932676b83ca998a390f3/
  kube-system_kube-controller-manager-5g1-comp2_57585a0305e4e46df816ebab263926f3/
  kube-system_kube-proxy-5g1-comp2_ce051fc91f9e463593a1d45efa60be52/
  kube-system_kube-scheduler-5g1-comp2_2495d4d1888db1561e78ccbc2ff8677c/
  kube-system_rke2-coredns-rke2-coredns-775c5b4bb4-8q4g2_e471d882-5bbb-479b-8410-043df2874e8c/
  kube-system_rke2-coredns-rke2-coredns-autoscaler-695fc554c9-rmwdh_20c5ecfc-001b-4873-8be1-88efd48cf14e/
  kube-system_rke2-ingress-nginx-controller-zrrrk_6eb4b396-3a98-4fc2-a70c-948568d759ec/
  kube-system_rke2-metrics-server-644f588b5-zqgq5_e9f421b5-56a0-47c5-a518-1b0c70d85b0d/
  kube-system_rke2-multus-ds-t2rq5_fee59f10-90d8-4a75-88f9-4b0369a62975/                                                               local-path-storage_local-path-provisioner-67f5f9cb7b-6cmqd_8d7929c5-c222-4de0-a482-1ecf00c78c4a/
  omec_amf-5887bbf6c5-kqs2p_b51e6386-d966-449c-8aa0-e12c1c444e01/
  omec_ausf-6dbb7655c7-ksknp_4114efa6-48e1-4c89-9d06-b8e6973209ae/
  omec_kafka-0_3d46935e-73a2-40ec-aa5d-3d0941268d6c/
  omec_metricfunc-55b47f58d5-2bf9x_630fb8c8-a91d-4ab5-9918-0065d515581b/                                                               omec_mongodb-0_b44d2466-79f3-47ff-b4ad-1bd9925459f0/
  omec_mongodb-1_ee8e3848-9eb4-46bc-a17f-eb1cd25fc4ba/
  omec_mongodb-arbiter-0_ff76bcdd-6721-47e7-90c8-a60ad6e55d6f/
  omec_nrf-54bf88c78c-2kcxb_cbc38cc7-208a-42b3-bf6f-479b347d6c0a/
  omec_nssf-5b85b8978d-dqj6n_a1a3bd68-49bc-4324-80d4-b5750d1904d1/
  omec_pcf-758d7cfb48-bzbgd_ff4542e8-33a6-48b4-b34c-1b3cdb893ab9/
  omec_sd-core-zookeeper-0_3d1d3095-9b76-4be5-8ce8-fb093e3ef03f/
  omec_simapp-6cccd6f787-gkj8s_4283acf8-f3ec-4184-a016-433362e5a4f7/
  omec_smf-b8f85d747-7lhhb_427c19bc-001d-4321-93af-7eb0951f6bc2/
  omec_udm-768b9987b4-wwxx8_d79f3a60-4416-40a6-a2bf-fd085784d005/
  omec_udr-8566897d45-mvrhk_bd95d72d-9ae7-4644-9c57-2dadbeff7923/
  omec_upf-0_0275fd22-8858-4fc1-9de5-117e4b5381d1/                                                                                     omec_webui-5894ffd49d-xcr6w_deacac1f-f1ad-4ca2-8234-3ef1ad682a8c/
  tigera-operator_tigera-operator-b77ddd45f-n76z8_824af4cd-f1d2-4a2f-864d-0deb219100e4/


  onfadmin@5g1-comp2:~/$ ll /var/log/pods/omec_amf-5887bbf6c5-kqs2p_b51e6386-d966-449c-8aa0-e12c1c444e01/amf/
  total 20M
  -rw-r--r-- 1 root root 659K Aug  4 12:46 0.log.20230804-120032.gz
  -rw-r--r-- 1 root root 660K Aug  4 13:32 0.log.20230804-124626.gz
  -rw-r----- 1 root root  11M Aug  4 14:18 0.log.20230804-141832                                                                       -rw-r--r-- 1 root root 659K Aug  4 14:18 0.log.20230804-133229.gz                                                                    -rw-r----- 1 root root 7.2M Aug  4 14:51 0.log

	--> seems 0.log is ongoing log being stored, 
		0.log.20230804-141832 is the file which is written before gz'ed, the number indicates timeline



---------------------------------
12. simple DNS test with busybox
---------------------------------
https://ranchermanager.docs.rancher.com/v2.5/troubleshooting/other-troubleshooting-tips/dns

(1) Check if domain names are resolving
  	kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 -- nslookup kubernetes.default

(2) Check if external names are resolving (in this example, www.google.com)
  	kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 -- nslookup www.google.com

(3) Check upstream nameservers in resolv.conf
  	kubectl run -i --restart=Never --rm test-${RANDOM} --image=ubuntu --overrides='{"kind":"Pod", "apiVersion":"v1", "spec": {"dnsPolicy":"Default"}}' -- sh -c 'cat /etc/resolv.conf'


(4) Check upstream nameservers in coredns container
  	(in case, host will run a local caching DNS nameserver, which means the address in /etc/resolv.conf will point to an address in the loopback range (127.0.0.0/8)
	  In case of Ubuntu 18.04, this is done by systemd-resolved. We detect if systemd-resolved is running, and will automatically use the /etc/resolv.conf file with the correct upstream nameservers (which is located at /run/systemd/resolve/resolv.conf).

	< A. Vagrant case >

	  vagrant@sdcore:~$ cat /run/systemd/resolve/resolv.conf
	  nameserver 10.0.2.3

	  vagrant@sdcore:~$ kubectl -n kube-system get configmap rke2-coredns-rke2-coredns -o go-template={{.data.Corefile}}
	  .:53 {
		  errors
		  health  {
			  lameduck 5s
		  }
		  ready
		  kubernetes   cluster.local  cluster.local in-addr.arpa ip6.arpa {
			  pods insecure
			  fallthrough in-addr.arpa ip6.arpa
			  ttl 30
		  }
		  prometheus   0.0.0.0:9153
		  forward   . /etc/resolv.conf
		  cache   30
		  loop
		  reload
		  loadbalance
	  }
	  
	vagrant@sdcore:~$ kubectl run -i --restart=Never --rm test-${RANDOM} --image=ubuntu --overrides='{"kind":"Pod", "apiVersion":"v1", "sec": {"dnsPolicy":"Default"}}' -- sh -c 'cat /etc/resolv.conf'
	  nameserver 10.0.2.3
	  pod "test-31269" deleted

	  vagrant@sdcore:~$ kubectl -n kube-system get pods -l k8s-app=kube-dns --no-headers -o custom-columns=NAME:.metadata.name,HOSTIP:.status.hostIP | while read pod host; do echo "Pod ${pod} on host ${host}"; kubectl -n kube-system exec $pod -c coredns -- cat /etc/resolv.c
	  onf; done
	  Pod rke2-coredns-rke2-coredns-775c5b4bb4-w4wzr on host 10.0.2.15
	  nameserver 10.0.2.3


	< B. 5g1-comp2 server >

	kubectl -n kube-system get pods -l k8s-app=kube-dns --no-headers -o custom-columns=NAME:.metadata.name,HOSTIP:.status.hostIP | while read pod host; do echo "Pod ${pod} on host ${host}"; kubectl -n kube-system exec $pod -c coredns -- cat /etc/resolv.conf; done
	Pod rke2-coredns-rke2-coredns-775c5b4bb4-8q4g2 on host 10.5.0.3
	search antd.nist.gov
	nameserver 10.5.1.1
	nameserver 10.5.9.200
	nameserver 10.0.20.200            




























