
Kubernetes Running Errors
=========================

1. no matches for kind "Deployment" in version "apps/v1beta1"

    in this case, run following first,

    $ kubectl api-resources | grep deployment
    deployments                       deploy       apps                           true  


    that means, that only apiVersion with apps is correct for Deployments (extensions is not supporting Deployment). 
    The same situation with StatefulSet.

    You need to change Deployment and StatefulSet apiVersion to apiVersion: apps/v1.




2. kubectl bash completion doesn't work in ubuntu docker container

    (1) 
    bash completion happens to generate the following error messages
    it doesn't work within the docker container "bash: _get_comp_words_by_ref: command not found."


    (2) Solution
    Install using yum install -y bash-completion and then 
    source using source /etc/profile.d/bash_completion.sh 
    followed by the actual command source <(kubectl completion bash)




3. docker change cgroup driver to systemd
(source:https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd)


    (1)
    errors:
    failed to run Kubelet: misconfiguration: kubelet cgroup driver: systemd is different from docker cgroup driver: cgroupfs


    (2)
    A solution that does not involve editing systemd units or drop-ins would be to create (or edit) the /etc/docker/daemon.json configuration file and to include the following:

    {
      "exec-opts": ["native.cgroupdriver=systemd"]
    }

    After saving it, restart your docker service.

    sudo systemctl restart docker
    This solution obviously is only feasible if you would want to apply this system-wide.


    -- OR --

   
    OS: Centos 7.4 As kubernetes 1.23.1 recommend to use cgroup systemd, and docker 20.10.20 use cgroup cgroupfs. 
    So, you have to change docker service file.

    step1: Stop docker service

    systemctl stop docker



    step2: change on files /etc/systemd/system/multi-user.target.wants/docker.service and /usr/lib/systemd/system/docker.service

    From :

    ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
    TO:

    ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd



    step3: start docker service and kubelet

    systemctl start docker
    kubeadm init phase kubelet-start 


4. kubelet - not start when 'systemctl start kubelet'

    (1) some errors regarding cni, "Unable to update cni config: No networks found in /etc/cni/net.d" 
        or "failed to load Kubelet config file /var/lib/kubelet/config.yaml"

        *CNI: container network interface

    (2) Solution:
        After 'kubeadm init [--pod-network-cidr 192.168.0.0/16]' 
        and "kubectl apply < cni >"
        will resolve this issues



5. If the restart of kubelet does not help, you can try to re-install the kubelet, it is a separate package:

    dnf(yum) reinstall kubelet                          on Fedora, Centos
    apt-get purge kubelet && apt-get install kubelet    on Debian/Ubuntu



6. firewalld is active, please ensure ports [6443 10250] are open or your cluster


    - install firewalld
    apt or dnf install firewalld

    - firewall-cmd --add-masquerade --permanent
    - firewall-cmd --reload 


    - on Master:
        -- firewall-cmd --zone=public --permanent --add-port={6443,2379,2380,10250,10251,10252}/tcp
        -- firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=<worker-IP-address>/32 accept' 
        -- firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=172.17.0.0/16 accept'
        -- firewall-cmd --reload

    - on Worker:
        -- firewall-cmd --zone=public --permanent --add-port={10250,30000-32767}/tcp
        -- firewall-cmd --reload
        



-------------------------------------------
7. taint error on calico-kube-controllers
-------------------------------------------
(Reference: https://waspro.tistory.com/563)

< Issue >
When installed kubernetes cluster by kubeadm and kubeinit

    vagrant@vagrant:~$ kubectl get po -A
    NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
    kube-system   calico-kube-controllers-6774bd9767-2ndxt   0/1     Pending   0          2m37s
    kube-system   calico-node-xtwjt                          1/1     Running   0          15m
    ...
    kube-system   kube-scheduler-vagrant                     1/1     Running   1          31m


--> calico-kube-controllers is pending status




(1) describing pod and 

    vagrant@vagrant:~$ kubectl describe pod calico-kube-controllers-6774bd9767-kll2p -n kube-system
    Name:                 calico-kube-controllers-6774bd9767-kll2p
    Namespace:            kube-system
    Priority:             2000000000
    Priority Class Name:  system-cluster-critical
    Node:                 <none>
    Labels:               k8s-app=calico-kube-controllers
                          pod-template-hash=6774bd9767
    Annotations:          <none>
    Status:               Pending
    IP:
    IPs:                  <none>
    Controlled By:        ReplicaSet/calico-kube-controllers-6774bd9767
    Containers:
      calico-kube-controllers:
        Image:      docker.io/calico/kube-controllers:v3.23.1
        Port:       <none>
        Host Port:  <none>
        Liveness:   exec [/usr/bin/check-status -l] delay=10s timeout=60s period=10s #success=1 #failure=6
        Readiness:  exec [/usr/bin/check-status -r] delay=0s timeout=60s period=10s #success=1 #failure=3
        Environment:
          ENABLED_CONTROLLERS:  node
          DATASTORE_TYPE:       kubernetes
        Mounts:
          /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2298w (ro)
    Conditions:
      Type           Status
      PodScheduled   False
    Volumes:
      kube-api-access-2298w:
        Type:                    Projected (a volume that contains injected data from multiple sources)
        TokenExpirationSeconds:  3607
        ConfigMapName:           kube-root-ca.crt
        ConfigMapOptional:       <nil>
        DownwardAPI:             true
    QoS Class:                   BestEffort
    Node-Selectors:              kubernetes.io/os=linux
    Tolerations:                 CriticalAddonsOnly op=Exists
                                 node-role.kubernetes.io/master:NoSchedule
                                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    Events:
      Type     Reason            Age                  From               Message
      ----     ------            ----                 ----               -------
      Warning  FailedScheduling  19s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.


--> status: pending
    Toleration is node-role.kubernetes.io/master:NoSchedule

--> to see events, but event are normally generted after pods are deployed
    So, let's see event directly

(2) investigation  for event  

    vagrant@vagrant:~$ kubectl get event -n kube-system
     TYPE      REASON               OBJECT                                          MESSAGE
     Warning   FailedScheduling     pod/calico-kube-controllers-56cdb7c587-qdx4p    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-56cdb7c587-qdx4p    skip schedule deleting pod: kube-system/calico-kube-controllers-56cdb7c587-qdx4p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-56cdb7c587   Created pod: calico-kube-controllers-56cdb7c587-qdx4p
     Normal    SuccessfulDelete     replicaset/calico-kube-controllers-56cdb7c587   Deleted pod: calico-kube-controllers-56cdb7c587-qdx4p
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-2ndxt    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-kll2p    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-kll2p    skip schedule deleting pod: kube-system/calico-kube-controllers-6774bd9767-kll2p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-6774bd9767   Created pod: calico-kube-controllers-6774bd9767-kll2p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-6774bd9767   Created pod: calico-kube-controllers-6774bd9767-2ndxt
     Normal    ScalingReplicaSet    deployment/calico-kube-controllers              Scaled up replica set calico-kube-controllers-56cdb7c587 to 1
     Normal    ScalingReplicaSet    deployment/calico-kube-controllers              Scaled down replica set calico-kube-controllers-56cdb7c587 to 0

--> untolerated taint



(3)
so, let's look at vagrant node as a master node


    vagrant@vagrant:~$ kubectl describe node vagrant
    Name:               vagrant
    ...
                        volumes.kubernetes.io/controller-managed-attach-detach: true
    CreationTimestamp:  Tue, 24 May 2022 19:26:20 +0000
    Taints:             node-role.kubernetes.io/control-plane:NoSchedule
    Unschedulable:      false


-->
Here, Tains, has the setting "NoSchedule" if the condition is "control-plane"
Because the vagrant node is a master node and kube system originally blocks calico controller from 
being initialized and running in the master node, so it has tained the node and set the NoSchedule
(https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)



< Solution >

At first, I did "kubectl taint nodes vagrant node-role.kubernetes.io/master-"
But not working...  ( --> it also needs to diable "-control-plane:NoSchedule")

After that, in here, (Reference: https://waspro.tistory.com/563),
Found good solution.

Getting rid of taint, add '-' at the end of command argument

    vagrant@vagrant:~$ kubectl taint nodes vagrant node-role.kubernetes.io/control-plane:NoSchedule-
    node/vagrant untainted

    vagrant@vagrant:~$ kubectl get po -A
    NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
    kube-system   calico-kube-controllers-6774bd9767-2ndxt   1/1     Running   0          21m
    kube-system   calico-node-xtwjt                          1/1     Running   0          35m
    kube-system   coredns-6d4b75cb6d-fn7rx                   1/1     Running   0          50m
    kube-system   coredns-6d4b75cb6d-xl979                   1/1     Running   0          50m
    kube-system   etcd-vagrant                               1/1     Running   1          50m
    kube-system   kube-apiserver-vagrant                     1/1     Running   1          50m
    kube-system   kube-controller-manager-vagrant            1/1     Running   0          50m
    kube-system   kube-proxy-bj5fm                           1/1     Running   0          50m
    kube-system   kube-scheduler-vagrant                     1/1     Running   1          50m


    ** Addtional disable taint command
    kubectl taint nodes vagrant node-role.kubernetes.io/master:NoSchedule-



-------------------------------------------
8. kubeadm init - Runtime container error 
-------------------------------------------

  vagrant@vagrant:~$ sudo kubeadm init
  [init] Using Kubernetes version: v1.24.0
  [preflight] Running pre-flight checks
          [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correct
  ly
  error execution phase preflight: [preflight] Some fatal errors occurred:
          [ERROR CRI]: container runtime is not running: output: time="2022-05-24T18:58:43Z" level=fatal msg="getting status of runtim
  e: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
  , error: exit status 1
  [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
  To see the stack trace of this error execute with --v=5 or higher


  (source: https://github.com/containerd/containerd/issues/4581)
  ** < Issue > **
  Following Kubernetes official installation instruction for containerd and kubeadm init will fail with
  unknown service runtime.v1alpha2.RuntimeService

  < possible cause ? maybe >
  W0524 16:59:01.427276 22679 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated
  and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket"
  with value "/var/run/dockershim.sock". Please update your configuration!


  ** < Solution > **
  rm /etc/containerd/config.toml
  systemctl restart containerd
  kubeadm init



-------------------------------------------
 9. docker ps shows no containers
-------------------------------------------
 
  < Possible Reason >
  When kubectl combinds docker together, and installed containerd
  endpoint indicates cri-o ..cri something


  < Issues >
  if you have following errors

  vagrant@vagrant:~$ crictl exec -it 3b69b6959a156 /bin/bash                                                                             
  WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
  ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory"
  ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: permission denied"
  ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/crio/crio.sock: connect: no such file or directory"
  ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/cri-dockerd.sock: connect: no such file or directory"
  FATA[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/cri-dockerd.sock: connect: no such file or directory"


  < Solution.1 >
    (https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/)

  vagrant@vagrant:~$ sudo crictl config --set runtime-endpoint=unix:///var/run/containerd/containerd.sock
  vagrant@vagrant:~$ sudo crictl exec -it 3b69b6959a156 /bin/bash
  [root@private-reg ~]#


  < Solution.2 >
  sudo -s (root account)
  export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml
  /var/lib/rancher/rke2/bin/crictl ps



------------------------------------------------------------
10. failed to run Kubelet: validate service connection: 
      CRI v1 runtime API is not implemented for endpoint
------------------------------------------------------------
(source: https://serverfault.com/questions/1118051/failed-to-run-kubelet-validate-service-connection-cri-v1-runtime-api-is-not-im)

(1) Issues

  - When "kubeadm init", following errors occurred 
 
	[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
	[kubelet-check] It seems like the kubelet isn't running or healthy. 
	...



  - to find the reason, used journalctl --> there 's failure to run kubelet because of k8s version up (1.26)

	  root@node0:~# journalctl -xeu kubelet
		...
		kubelet[17178]: E0107 17:00:47.601541   17178 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/...
		...



  	Main Cause was that "the removal of the CRI v1alpha2 API and containerd 1.5 support" in K8s 1.26
	(https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/#cri-api-removal)



(2) Solutions for Ubuntu 18 or 20

  The solution would be to install containerd 1.6, but... there doesn't seem to be an APT package for that yet :( . 
  I was only able to find 1.6+ packages for Ubuntu 22.10 and above.

	A. The one listed in the link above - running an older version of the kubelet (1.25)

		  apt remove --purge kubeadm kubelet
		  apt install -y kubeadm kubelet=1.25.5-00


	B. Manually upgrading containerd to 1.6 or above, by downloading and replacing the binaries

		  wget https://github.com/containerd/containerd/releases/download/v1.6.12/containerd-1.6.12-linux-amd64.tar.gz
		  tar xvf containerd-1.6.12-linux-amd64.tar.gz
		  sudo systemctl stop containerd
		  cd bin
		  sudo cp * /usr/bin/
		  sudo systemctl start containerd


	C. In the Docker repos, there are packages for containerd 1.6 and above. 
	   So you can also add the Docker repos, and install containerd.io from there:

		  sudo mkdir -p /etc/apt/keyrings
		  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
		  echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg]  https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
		  sudo apt update
		  sudo apt install containerd.io -y




-------------------------------------------------------
11. Kubelet not Starting, no Connection to API Server
-------------------------------------------------------
(https://admantium.com/blog/kube11_k8s_with_kubeadm_tutorial/)

[issues]
"Unable to register node with API server" err="Post \"https://167.235.73.16:6443/api/v1/nodes\": dial tcp 167.235.73.16:6443: connect: connection refused" node="kubeadm-master"


< solution >
No containers are started. I followed the kubeadm documentation and used the provided container-d configuration. But this turned out to be wrong. After several attempts, trying configurations on Github, I ended up with this /etc/containerd/config.toml:

version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
   [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true




-------------------------------------------
12.  still not see a starting container
-------------------------------------------
(https://admantium.com/blog/kube11_k8s_with_kubeadm_tutorial/)

[error]
 error="failed to create containerd container: get apparmor_parser version: exec: \"apparmor_parser\": executable file not found in $PATH"


[Solution]
> apt install apparmor apparmor-utils



---------------------------------------------
13. Persist IPv4 and Bridge Network Settings
---------------------------------------------
(https://admantium.com/blog/kube11_k8s_with_kubeadm_tutorial/)


# Source: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic
> cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

> sudo modprobe overlay
> sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
> cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
> sudo sysctl --system




-----------------------------------
14. "unknown field "serviceName" 
-----------------------------------

(https://stackoverflow.com/questions/64125048/get-error-unknown-field-servicename-in-io-k8s-api-networking-v1-ingressbacken)


  paths:
  - path: /
	backend:
	  serviceName: test-app
	  servicePort: 5000

since 1.19 cluster, it sould be fixed like following, 

	backend:
	  service:
		name: test
		port:
		  number: 80






---------------
ETC
---------------

kubeadm reset : when need to stop, reset kubeadm init



















