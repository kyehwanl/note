
Kubernetes Running Errors
=========================

1. no matches for kind "Deployment" in version "apps/v1beta1"

    in this case, run following first,

    $ kubectl api-resources | grep deployment
    deployments                       deploy       apps                           true  


    that means, that only apiVersion with apps is correct for Deployments (extensions is not supporting Deployment). 
    The same situation with StatefulSet.

    You need to change Deployment and StatefulSet apiVersion to apiVersion: apps/v1.




2. kubectl bash completion doesn't work in ubuntu docker container

    (1) 
    bash completion happens to generate the following error messages
    it doesn't work within the docker container "bash: _get_comp_words_by_ref: command not found."


    (2) Solution
    Install using yum install -y bash-completion and then 
    source using source /etc/profile.d/bash_completion.sh 
    followed by the actual command source <(kubectl completion bash)




3. docker change cgroup driver to systemd
(source:https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd)


    (1)
    errors:
    failed to run Kubelet: misconfiguration: kubelet cgroup driver: systemd is different from docker cgroup driver: cgroupfs


    (2)
    A solution that does not involve editing systemd units or drop-ins would be to create (or edit) the /etc/docker/daemon.json configuration file and to include the following:

    {
      "exec-opts": ["native.cgroupdriver=systemd"]
    }

    After saving it, restart your docker service.

    sudo systemctl restart docker
    This solution obviously is only feasible if you would want to apply this system-wide.


    -- OR --

   
    OS: Centos 7.4 As kubernetes 1.23.1 recommend to use cgroup systemd, and docker 20.10.20 use cgroup cgroupfs. 
    So, you have to change docker service file.

    step1: Stop docker service

    systemctl stop docker



    step2: change on files /etc/systemd/system/multi-user.target.wants/docker.service and /usr/lib/systemd/system/docker.service

    From :

    ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
    TO:

    ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd



    step3: start docker service and kubelet

    systemctl start docker
    kubeadm init phase kubelet-start 


4. kubelet - not start when 'systemctl start kubelet'

    (1) some errors regarding cni, "Unable to update cni config: No networks found in /etc/cni/net.d" 
        or "failed to load Kubelet config file /var/lib/kubelet/config.yaml"

        *CNI: container network interface

    (2) Solution:
        After 'kubeadm init [--pod-network-cidr 192.168.0.0/16]' 
        and "kubectl apply < cni >"
        will resolve this issues



5. If the restart of kubelet does not help, you can try to re-install the kubelet, it is a separate package:

    dnf(yum) reinstall kubelet                          on Fedora, Centos
    apt-get purge kubelet && apt-get install kubelet    on Debian/Ubuntu



6. firewalld is active, please ensure ports [6443 10250] are open or your cluster


    - install firewalld
    apt or dnf install firewalld

    - firewall-cmd --add-masquerade --permanent
    - firewall-cmd --reload 


    - on Master:
        -- firewall-cmd --zone=public --permanent --add-port={6443,2379,2380,10250,10251,10252}/tcp
        -- firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=<worker-IP-address>/32 accept' 
        -- firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=172.17.0.0/16 accept'
        -- firewall-cmd --reload

    - on Worker:
        -- firewall-cmd --zone=public --permanent --add-port={10250,30000-32767}/tcp
        -- firewall-cmd --reload
        



-------------------------------------------
7. aaint error on calico-kube-controllers
-------------------------------------------
(Reference: https://waspro.tistory.com/563)

< Issue >
When installed kubernetes cluster by kubeadm and kubeinit

    vagrant@vagrant:~$ kubectl get po -A
    NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
    kube-system   calico-kube-controllers-6774bd9767-2ndxt   0/1     Pending   0          2m37s
    kube-system   calico-node-xtwjt                          1/1     Running   0          15m
    ...
    kube-system   kube-scheduler-vagrant                     1/1     Running   1          31m


--> calico-kube-controllers is pending status




(1) describing pod and 

    vagrant@vagrant:~$ kubectl describe pod calico-kube-controllers-6774bd9767-kll2p -n kube-system
    Name:                 calico-kube-controllers-6774bd9767-kll2p
    Namespace:            kube-system
    Priority:             2000000000
    Priority Class Name:  system-cluster-critical
    Node:                 <none>
    Labels:               k8s-app=calico-kube-controllers
                          pod-template-hash=6774bd9767
    Annotations:          <none>
    Status:               Pending
    IP:
    IPs:                  <none>
    Controlled By:        ReplicaSet/calico-kube-controllers-6774bd9767
    Containers:
      calico-kube-controllers:
        Image:      docker.io/calico/kube-controllers:v3.23.1
        Port:       <none>
        Host Port:  <none>
        Liveness:   exec [/usr/bin/check-status -l] delay=10s timeout=60s period=10s #success=1 #failure=6
        Readiness:  exec [/usr/bin/check-status -r] delay=0s timeout=60s period=10s #success=1 #failure=3
        Environment:
          ENABLED_CONTROLLERS:  node
          DATASTORE_TYPE:       kubernetes
        Mounts:
          /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2298w (ro)
    Conditions:
      Type           Status
      PodScheduled   False
    Volumes:
      kube-api-access-2298w:
        Type:                    Projected (a volume that contains injected data from multiple sources)
        TokenExpirationSeconds:  3607
        ConfigMapName:           kube-root-ca.crt
        ConfigMapOptional:       <nil>
        DownwardAPI:             true
    QoS Class:                   BestEffort
    Node-Selectors:              kubernetes.io/os=linux
    Tolerations:                 CriticalAddonsOnly op=Exists
                                 node-role.kubernetes.io/master:NoSchedule
                                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    Events:
      Type     Reason            Age                  From               Message
      ----     ------            ----                 ----               -------
      Warning  FailedScheduling  19s (x2 over 5m41s)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.


--> status: pending
    Toleration is node-role.kubernetes.io/master:NoSchedule

--> to see events, but event are normally generted after pods are deployed
    So, let's see event directly

(2) investigation  for event  

    vagrant@vagrant:~$ kubectl get event -n kube-system
     TYPE      REASON               OBJECT                                          MESSAGE
     Warning   FailedScheduling     pod/calico-kube-controllers-56cdb7c587-qdx4p    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-56cdb7c587-qdx4p    skip schedule deleting pod: kube-system/calico-kube-controllers-56cdb7c587-qdx4p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-56cdb7c587   Created pod: calico-kube-controllers-56cdb7c587-qdx4p
     Normal    SuccessfulDelete     replicaset/calico-kube-controllers-56cdb7c587   Deleted pod: calico-kube-controllers-56cdb7c587-qdx4p
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-2ndxt    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-kll2p    0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
     Warning   FailedScheduling     pod/calico-kube-controllers-6774bd9767-kll2p    skip schedule deleting pod: kube-system/calico-kube-controllers-6774bd9767-kll2p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-6774bd9767   Created pod: calico-kube-controllers-6774bd9767-kll2p
     Normal    SuccessfulCreate     replicaset/calico-kube-controllers-6774bd9767   Created pod: calico-kube-controllers-6774bd9767-2ndxt
     Normal    ScalingReplicaSet    deployment/calico-kube-controllers              Scaled up replica set calico-kube-controllers-56cdb7c587 to 1
     Normal    ScalingReplicaSet    deployment/calico-kube-controllers              Scaled down replica set calico-kube-controllers-56cdb7c587 to 0

--> untolerated taint



(3)
so, let's look at vagrant node as a master node


    vagrant@vagrant:~$ kubectl describe node vagrant
    Name:               vagrant
    ...
                        volumes.kubernetes.io/controller-managed-attach-detach: true
    CreationTimestamp:  Tue, 24 May 2022 19:26:20 +0000
    Taints:             node-role.kubernetes.io/control-plane:NoSchedule
    Unschedulable:      false


-->
Here, Tains, has the setting "NoSchedule" if the condition is "control-plane"
Because the vagrant node is a master node and kube system originally blocks calico controller from 
being initialized and running in the master node, so it has tained the node and set the NoSchedule
(https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)



< Solution >

At first, I did "kubectl taint nodes vagrant node-role.kubernetes.io/master-"
But not working...

After that, in here, (Reference: https://waspro.tistory.com/563),
Found good solution.

Getting rid of taint, add '-' at the end of command argument

    vagrant@vagrant:~$ kubectl taint nodes vagrant node-role.kubernetes.io/control-plane:NoSchedule-
    node/vagrant untainted

    vagrant@vagrant:~$ kubectl get po -A
    NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
    kube-system   calico-kube-controllers-6774bd9767-2ndxt   1/1     Running   0          21m
    kube-system   calico-node-xtwjt                          1/1     Running   0          35m
    kube-system   coredns-6d4b75cb6d-fn7rx                   1/1     Running   0          50m
    kube-system   coredns-6d4b75cb6d-xl979                   1/1     Running   0          50m
    kube-system   etcd-vagrant                               1/1     Running   1          50m
    kube-system   kube-apiserver-vagrant                     1/1     Running   1          50m
    kube-system   kube-controller-manager-vagrant            1/1     Running   0          50m
    kube-system   kube-proxy-bj5fm                           1/1     Running   0          50m
    kube-system   kube-scheduler-vagrant                     1/1     Running   1          50m









